# Multi-LLM Collective Memory Research Framework

## ğŸ§  ExploraciÃ³n de la Emergencia de Memoria Colectiva Distribuida en Sistemas Multi-LLM

Este repositorio contiene el framework experimental para investigar el impacto de la memoria colaborativa persistente entre mÃºltiples agentes LLM en tareas complejas de razonamiento multietapa.

### ğŸ“Œ HipÃ³tesis Principal

La memoria colaborativa persistente entre mÃºltiples agentes LLM puede mejorar significativamente la eficiencia y precisiÃ³n de tareas complejas de razonamiento multietapa bajo restricciones de contexto, en comparaciÃ³n con memorias aisladas o ausencia de memoria.

### ğŸ—ï¸ Estructura del Proyecto

```
â”œâ”€â”€ paper/                    # Paper de investigaciÃ³n y documentaciÃ³n
â”œâ”€â”€ experiments/             # Configuraciones experimentales
â”œâ”€â”€ agents/                  # ImplementaciÃ³n de agentes especializados
â”œâ”€â”€ memory_systems/          # Sistemas de memoria (compartida, privada, sin memoria)
â”œâ”€â”€ benchmarks/             # Tareas de evaluaciÃ³n y mÃ©tricas
â”œâ”€â”€ evaluation/             # Scripts de evaluaciÃ³n y anÃ¡lisis
â”œâ”€â”€ results/                # Resultados experimentales y visualizaciones
â””â”€â”€ docker/                 # ConfiguraciÃ³n de contenedores
```

### ğŸš€ Inicio RÃ¡pido

1. **Configurar entorno:**
   ```bash
   docker-compose up -d
   ```

2. **Ejecutar experimento bÃ¡sico:**
   ```bash
   python experiments/run_benchmark.py --config configs/basic_comparison.yaml
   ```

3. **Analizar resultados:**
   ```bash
   python evaluation/analyze_results.py --experiment_id latest
   ```

### ğŸ“Š Configuraciones Experimentales

- **Single Agent**: GPT-4 solo, sin memoria externa (baseline)
- **Multi-Agente sin memoria**: MÃºltiples LLMs con solo historial de diÃ¡logo
- **Multi-Agente con memoria compartida**: Memoria comÃºn persistente JSON/vector DB
- **Multi-Agente con memoria privada**: Cada agente con memoria individual aislada

### ğŸ¯ MÃ©tricas de EvaluaciÃ³n

- Accuracy del resultado final
- NÃºmero de tokens totales utilizados
- Redundancia computacional evitada
- Tasa de reutilizaciÃ³n de conocimientos
- UtilizaciÃ³n de memoria (lecturas/escrituras)
- Calidad incremental del output

### ğŸ“ Contribuciones

Este trabajo busca llenar el vacÃ­o en benchmarks estÃ¡ndar que cuantifiquen la contribuciÃ³n especÃ­fica de la memoria distribuida compartida en tareas cognitivas multi-LLM.

---

**Autor**: [Tu nombre]  
**InstituciÃ³n**: [Tu instituciÃ³n]  
**Contacto**: [Tu email]
